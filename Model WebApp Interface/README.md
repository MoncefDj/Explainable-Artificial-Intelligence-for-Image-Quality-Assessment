# Image Quality Assessment - AI Powered Tool
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)

A Flask-based AI tool for analyzing X-ray images using DeepLabV3+ segmentation and saliency-based object scoring. Includes a rich web interface with real-time visual feedback and optional AI explanations.

## Project Overview

The Image Quality Assessment tool is a web application designed to analyze X-ray images for foreign objects and assess their overall quality. It leverages a PyTorch-based deep learning model (DeepLabV3+) for image segmentation and employs various image analysis techniques to score detected objects based on size, location, and model confidence (saliency). The application provides visual feedback, a detailed summary report, and an AI-generated explanation (via GPT4All) of the findings.

The frontend is built with HTML, CSS (Bootstrap 5), and vanilla JavaScript, while the backend is powered by Flask (Python). `pyngrok` is integrated to optionally expose the local development server publicly for temporary sharing.

## Features

*   **Image Index Selection:** Users can select an X-ray image from a predefined dataset by its index.
*   **Automated Segmentation:** Utilizes a DeepLabV3+ model to segment potential foreign objects in the selected X-ray.
*   **Saliency Mapping:** Generates NormGrad-based saliency maps to highlight regions the model deems important for its segmentation decision.
*   **Object Scoring:**
    *   Detects connected components from the segmentation mask.
    *   Scores each detected object based on:
        *   Relative size (with a sigmoid penalty for very small or very large objects).
        *   Location importance (central regions are typically more critical).
        *   Saliency confidence (mean saliency value within the object's bounds).
*   **Quality Metrics:**
    *   Calculates an overall image quality score based on all detected objects.
    *   Calculates a filtered image quality score based only on high-confidence objects.
*   **Visual Outputs:**
    *   Displays the original X-ray image.
    *   Shows an overlay of the segmentation mask with object bounding boxes.
    *   Presents a heatmap overlay of the saliency map with object bounding boxes colored by confidence.
*   **Detailed Reports:**
    *   **Analysis Report:** A structured text summary of the analysis parameters, scoring methodology, object details, and quality scores. Supports LaTeX rendering for mathematical formulas.
    *   **AI-Generated Explanation:** (Optional, if LLM is enabled and configured) Provides a natural language explanation of the analysis results, generated by a local LLM (GPT4All).
*   **User Interface:**
    *   Responsive design using Bootstrap 5.
    *   Dark mode toggle for user preference.
    *   Progress bar during analysis.
    *   Clear presentation of results in tabs.
*   **Public Sharing (Optional):** Integrated `pyngrok` to create a temporary public URL for the local development server.

## Technology Stack

*   **Backend:**
    *   Python 3.x
    *   Flask (Web framework)
    *   PyTorch (Deep learning framework)
    *   segmentation-models-pytorch (Pre-trained segmentation models)
    *   OpenCV (Image processing)
    *   NumPy, Pandas (Data manipulation)
    *   Matplotlib (For generating image overlays server-side)
    *   Albumentations (Image augmentation - used for dataset preparation)
    *   pyngrok (Python wrapper for ngrok)
    *   GPT4All (For local LLM, optional)
*   **Frontend:**
    *   HTML5
    *   CSS3 (with Bootstrap 5)
    *   Vanilla JavaScript
    *   Marked.js (Markdown to HTML rendering)
    *   MathJax (LaTeX rendering)
    *   Highlight.js (Code syntax highlighting in Markdown)
    *   Font Awesome (Icons)

    ## Project Structure
        xray_analysis_webapp/
        â”œâ”€â”€ app.py                          # Flask backend with all Python logic
        â”œâ”€â”€ static/
        â”‚   â”œâ”€â”€ css/
        â”‚   â”‚   â””â”€â”€ style.css              # Custom styles and dark mode support
        â”‚   â”œâ”€â”€ js/
        â”‚   â”‚   â””â”€â”€ script.js              # Frontend interactivity and API calls
        â”‚   â””â”€â”€ images/
        â”‚       â””â”€â”€ logo.png              # Favicon and navbar logo (replace as needed)
        â”œâ”€â”€ templates/
        â”‚   â””â”€â”€ index.html                 # Main HTML template
        â”œâ”€â”€ data/                          # Dataset directory (configured in app.py)
        â”‚   â””â”€â”€ object-CXR/
        â”‚       â”œâ”€â”€ dev.csv               # Metadata CSV for images
        â”‚       â””â”€â”€ dev/                  # Image files corresponding to dev.csv
        â”œâ”€â”€ models/                        # Trained model directory
        â”‚   â””â”€â”€ focal_tversky_loss_best_model.pth  # Trained model weights
        â””â”€â”€ requirements.txt               # List of Python dependencies


## Setup and Installation

1.  **Clone the Repository (if applicable) or Create Project Directory:**
    ```bash
    # If cloning:
    # git clone <repository-url>
    # cd xray_analysis_webapp
    # If creating manually:
    mkdir xray_analysis_webapp
    cd xray_analysis_webapp
    mkdir -p static/css static/js static/images templates
    ```

2.  **Place Project Files:**
    *   Copy `app.py` into the `xray_analysis_webapp/` root.
    *   Copy `index.html` into `xray_analysis_webapp/templates/`.
    *   Copy `style.css` into `xray_analysis_webapp/static/css/`.
    *   Copy `script.js` into `xray_analysis_webapp/static/js/`.
    *   Create/place your `logo.png` (and optionally `favicon-16x16.png`, `favicon-32x32.png`, `apple-touch-icon.png`) in `xray_analysis_webapp/static/images/`.

3.  **Configure Paths in `app.py`:**
    Open `app.py` and verify/update the following paths to match your system:
    ```python
    DATA_BASE_PATH = "/path/to/your/object-CXR_folder" 
    TRAINED_MODEL_PATH = "/path/to/your/focal_tversky_loss_best_model.pth"
    ```
    The current defaults are:
    ```python
    DATA_BASE_PATH = "/home/linati/SegmToClass/Train/object-CXR"
    TRAINED_MODEL_PATH = "/home/linati/SegmToClass/Train/loss_experiment_outputs/focal_tversky_loss/focal_tversky_loss_best_model.pth"
    ```
    Ensure your `dev.csv` and corresponding image folder (e.g., `dev/`) are within the `DATA_BASE_PATH`.

4.  **Set up Python Virtual Environment:**
    It is highly recommended to use a virtual environment.
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Linux/macOS
    # venv\Scripts\activate    # On Windows
    ```

5.  **Install System-Level Dependencies (Linux):**
    These are required for OpenCV, Matplotlib, and other libraries. For Debian/Ubuntu based systems:
    ```bash
    sudo apt update
    sudo apt install -y \
        libgl1-mesa-glx \
        libglib2.0-0 \
        libsm6 \
        libxext6 \
        libxrender1 \
        libfontconfig1 \
        libice6
    ```
    For other Linux distributions, install the equivalent packages.

6.  **Install Python Dependencies:**
    Create a `requirements.txt` file in the root of your project (`xray_analysis_webapp/`) with the following content:
    ```txt
    Flask
    torch
    torchvision
    opencv-python-headless
    pandas
    albumentations
    segmentation-models-pytorch
    matplotlib
    Pillow
    gpt4all
    numpy
    pyngrok
    ```
    Then install them:
    ```bash
    pip install -r requirements.txt
    ```
    *Note on PyTorch: If you have a CUDA-enabled GPU and want to use it, ensure you install the correct PyTorch version with CUDA support. Visit [pytorch.org](https://pytorch.org/get-started/locally/) for specific commands.*

7.  **(Optional) `ngrok` Authtoken for Public URL:**
    If you want to use `pyngrok` to share your app publicly:
    *   Sign up at [ngrok.com](https://ngrok.com) and get your authtoken.
    *   Open `app.py` and replace `"YOUR_NGROK_AUTH_TOKEN"` with your actual token:
        ```python
        NGROK_AUTH_TOKEN = "YOUR_ACTUAL_NGROK_AUTHTOKEN_HERE"
        ```
    If you don't set this, `ngrok` will not start, and the app will only be accessible locally.

## Running the Application

1.  **Navigate to the Project Directory:**
    ```bash
    cd /path/to/your/xray_analysis_webapp
    ```
3.  **Run the Flask App:**
    ```bash
    python app.py
    ```
3.  **Access in Browser:**
    *   **Locally:** Open your web browser and go to `http://localhost:5000` or `http://127.0.0.1:5000`.
    *   **On Local Network:** Other devices on the same network can access it using your machine's local IP address (e.g., `http://<your-local-ip>:5000`). Find your local IP with `hostname -I` or `ip addr show` on Linux.
    *   **Publicly (if ngrok is configured and started successfully):** The console output from `python app.py` will show an `ngrok.io` URL. Use this URL to access the application from any network.

## Using the Tool

1.  The application will load, and the "Dataset Status" should indicate if the dataset was found and the number of images.
2.  Enter an **Image Index** number in the input field (within the displayed valid range).
3.  Optionally, toggle the "Enable AI Explanation" checkbox if the LLM is available and you want a textual explanation.
4.  Click the "**Start Analysis**" button.
5.  A progress bar will show the analysis status.
6.  Once complete, the results will be displayed:
    *   Quality scores.
    *   Original, Segmentation Overlay, and Saliency Overlay images.
    *   A detailed "Analysis Report" tab (with Markdown and LaTeX rendering).
    *   An "AI Explanation" tab if enabled and successful.
7.  Use the "Dark Mode" toggle in the navbar to switch themes.

## Potential Future Enhancements

*   **Image Upload:** Allow users to upload their own X-ray images instead of selecting from a predefined dataset.
*   **Interactive Image Exploration:** Add zoom/pan capabilities to the displayed images.
*   **Advanced Configuration:** Allow users to tweak analysis parameters (e.g., sigmoid values, weights) via the UI.
*   **Real-time Progress Updates:** Implement WebSockets or Server-Sent Events for more granular progress updates from the backend during analysis.
*   **User Accounts & History:** Allow users to save and review past analyses.
*   **More Robust Error Handling:** Implement more user-friendly error messages and logging.
*   **Production Deployment:** Deploy using a production-grade WSGI server (like Gunicorn or uWSGI) and a reverse proxy (like Nginx) for better performance and security if deploying publicly for extended periods.
*   **Model Management:** Interface to select different trained models.
*   **Batch Processing:** Ability to analyze multiple images at once.

## ðŸ“„ License

This project is licensed under the [MIT License](./LICENSE).

